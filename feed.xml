<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://woka-inc.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://woka-inc.github.io/" rel="alternate" type="text/html" /><updated>2025-02-24T07:04:37+00:00</updated><id>https://woka-inc.github.io/feed.xml</id><title type="html">Woka 기술 블로그</title><subtitle>생존을 위한 선택이 아닌 필수, DX를 넘어 AX로 가는 길</subtitle><author><name>Woka</name></author><entry><title type="html">순환신경망(Recurrent Neural Network, RNN)</title><link href="https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2025/01/31/RNN.html" rel="alternate" type="text/html" title="순환신경망(Recurrent Neural Network, RNN)" /><published>2025-01-31T00:00:00+00:00</published><updated>2025-01-31T00:00:00+00:00</updated><id>https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2025/01/31/RNN</id><content type="html" xml:base="https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2025/01/31/RNN.html"><![CDATA[<p><a href="https://woka-inc.github.io/deep%20learning%20기초/2024/12/09/ANN-2.html">앞선 글</a>에서 알아본 전통적인 인공 신경망은 데이터 속 패턴을 학습해 새로운 값을 예측해 낼 수 있습니다. 순전파를 통해 임의의 가중치로 모델의 예측값을 계산해 내고, 역전파를 통해 예측값과 정답 사이의 오차를 줄이는 방향으로 가중치를 업데이트했습니다. 이 모든 과정이 데이터의 패턴을 가장 잘 나타내는 가중치를 찾아내는 학습이었죠. 하지만 이 전통적인 인공 신경망은 순서가 있는 <strong>순차 데이터(sequential data, 이하 시퀀스 혹은 시퀀스 데이터)</strong>를 학습하기에 적합하지 않습니다. 이번 글에서는 이 시퀀스를 다루는 시퀀스 모델 중 가장 기본적인 인공신경망 모델인 <strong>순환 신경망(Recurrent Neural Network, 이하 RNN)</strong>을 전통적인 ANN과 비교하며 알아보고, tensorflow로 간단히 구현한 후, 어떤 한계가 남아있는지 소개하겠습니다.</p>

<h2 id="대표적인-순차-데이터-자연어">대표적인 순차 데이터, 자연어</h2>

<p>시퀀스 데이터에는 여러 가지가 있습니다. 우리가 하는 말인 자연어, 소리를 표현한 음파, 주식 거래량, 교통량 등처럼 같은 유형의 값이 나열되어 있지만 값의 순서에 따라 그 의미가 달라지는 데이터들을 <strong>순차 데이터 혹은 시퀀스</strong>라고 합니다. 그중에서도 시간의 흐름에 따라 의미가 존재하는 데이터는 <strong>시계열 데이터(time-series data)</strong>라고 합니다.</p>

<p>여러 유형의 순차 데이터가 있지만, 오늘 RNN을 설명하면서 다룰 순차 데이터는 바로 자연어입니다. 자연어는 사람이 일상에서 쓰는 언어로, 같은 단어도 그 의미가 애매하거나 유연하다는 특징이 있습니다. 컴퓨터에 명령을 내릴 때는, 0과 1만 이해할 수 있는 컴퓨터와 사람이 소통하기 위한 문법의 일종인 고급 프로그래밍 언어를 사용해 오고 있습니다. 만약 컴퓨터가 사람의 언어인 자연어를 이해한다면 고급 프로그래밍 언어를 배우지 않은 사람도 컴퓨터에 다양한 작업을 지시할 수 있겠죠? 이를 가능하게 하는 기술을 연구하는 분야를 <strong>자연어 처리(Natural Language Processing, NLP)</strong>라고 합니다.</p>

<p>자연어 처리에서는 다양한 <strong>언어 모델(Language Model)</strong>이 활용됩니다. 언어 모델은 단어 시퀀스인 문장의 확률을 계산합니다. 이 확률은 해당 문장이 적당히 자연스러워서 실제로 일어날 확률을 뜻합니다. 예를 들어 “No pain no gain.”이라는 문장의 확률은 0.7, “No pain spooky red.”라는 문장이 일어날 확률은 0.01입니다. “No pain” 뒤에 이어질 내용으로 “no gain”과 “spooky red”라는 후보 문장을 만든 후, 더 높은 확률을 가진 문장을 출력하는 거죠.</p>

<p>언어 모델에서 단어를 처리하기 위해서는 단어를 <strong>벡터(vector)</strong>로 표현해야 합니다. 벡터는 컴퓨터가 이해할 수 있는 숫자로 구성되어 있죠. 단어를 벡터로 표현하는 방법에는 여러 가지가 있습니다. 가장 기본적인 방법의 하나는 단어 집합의 크기만큼 벡터 차원을 설정하고, 표현하려는 단어의 인덱스에는 1을, 나머지에는 0을 할당하는 <strong>원-핫 인코딩(one-hot encoding)</strong>입니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/No-pain-no-gain의-원-핫-인코딩.png" alt="No pain No gain의 원-핫 인코딩" width="400" /></p>
<p class="img_caption">No pain no gain의 원-핫 인코딩</p>

<p>위 그림에서처럼 <em>No pain no gain</em>의 <em>pain</em>은 [0, 1, 0]으로 원-핫 인코딩 될 수 있습니다.</p>

<p>하지만 이 방법은 단어 간의 의미적 관계를 반영하지 못한다는 한계가 있습니다. 우리가 새로운 단어를 공부할 때 그 단어가 사용되는 여러 문장을 접하면서 단어의 의미를 구체화해 나가는 것처럼, 컴퓨터도 서로 다른 단어가 언제 함께 사용되는지, 혹은 의미상으로 유사한지 정도를 파악하면 그 단어를 보다 잘 이해하고 사용할 수 있습니다.</p>

<p>이 아이디어를 토대로 단어의 의미를 충분히 반영해 다차원 공간에 벡터화하는 것을 <strong>단어의 분산 표현(distributional representation)</strong>이라고 하며, 이 과정을 <strong>단어 임베딩(word embedding)</strong>이라고 합니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/단어-임베딩.png" alt="단어 임베딩" width="800" /></p>
<p class="img_caption">단어 임베딩
    <a class="img_ref" href="https://medium.com/artists-and-machine-intelligence/ami-residency-part-1-exploring-word-space-andprojecting-meaning-onto-noise-98af7252f749">(출처)</a>
</p>

<p>단어를 의미적으로 벡터화하면, 위 그림에서처럼 유사한 관계를 가진 단어 쌍들이 비슷한 방향성을 나타냅니다. 예를 들어, <em>man</em>에서 <em>woman</em>으로 향하는 벡터와 <em>king</em>에서 <em>queen</em>으로 향하는 벡터가 유사한 패턴을 보이는 것처럼요.</p>

<p>RNN이 학습하는 데이터는 이러한 분산 표현으로 변환된 단어 벡터들로 구성됩니다. 이번 글에서는 RNN이 학습할 자연어 단어들이 단어 임베딩 과정을 거쳐 분산 표현 벡터로 모델에게 전달된다는 점을 이해하는 데 집중하며, 단어 임베딩의 구체적인 구현 방법은 추후 자연어 처리를 따로 다루며 자세히 설명하겠습니다.</p>

<h2 id="rnn의-구조">RNN의 구조</h2>
<p>앞서 배운 ANN은 feed forward 신경망입니다. Feed forward는 입력층부터 출력층까지 모델의 순전파 과정이 한 방향으로만 진행되는 방식을 뜻합니다. RNN과 feed forward ANN의 구조를 비교해 보면 왜 RNN이 문장 학습에 더 적합한지 그 이유를 알 수 있습니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/RNN과-feed-forward-ANN.png" alt="RNN과 feed forward ANN" width="" /></p>
<p class="img_caption">RNN과 feed forward ANN</p>

<p>RNN의 ‘R’은 ‘되풀이되는’이라는 뜻을 가진 <em>recurrent</em>의 약자입니다. 모델의 데이터 흐름이 한 방향만을 가리키는 feed forward ANN과 달리 RNN에서는 <em>recurrent</em>를 보여주는 화살표가 있죠. 여기서 $t$는 time(특정 시점)을 의미합니다. N개의 순서가 있는 데이터를 입력한다면 $t$는 0부터 N-1까지 데이터를 순차적으로 할당받고, 매번 동일한 뉴런을 순환하며 데이터를 처리합니다.</p>

<p>RNN의 가장 큰 특징은 그림처럼 하나의 뉴런을 순환하는 구조를 가지고 있다는 점입니다. RNN의 순환 구조를 돌돌 말린 김밥을 펼치듯 펼쳐 전체 구조(full network)를 보여주는 것을 <strong>unfold</strong>라고 합니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/No-pain-no-gain을-입력받는-RNN-펼치기.png" alt="“No pain no gain”을 입력받는 RNN 펼치기" width="" /></p>
<p class="img_caption">"No pain no gain"을 입력받는 RNN 펼치기</p>

<p>앞서 살펴본 RNN의 순환구조를 반시계 방향으로 90도 돌린 후 펼친 그림입니다. 펼치기 전 $x_t$, $h_t$, $o_t$는 $t$ 시점의 값을 나타냈습니다. <em>No pain no gain</em>이라는 문장을 입력받는다면, 이 RNN을 펼쳤을 때 각 단어에 대해 순환하므로 총 4번의 순환이 진행됩니다.</p>

<h3 id="x_t-t-시점의-입력값">$x_t$: $t$ 시점의 입력값</h3>

<p>$x$는 ANN에서와 같이 입력값을 나타냅니다. $x_t$는 t시점에서의 입력값이죠. 입력값이 자연어인 경우에는 t번째 단어의 분산 처리 벡터가 입력됩니다. 예를 들어 모델에 <em>No pain no gain</em>이라는 문장을 입력하면, $x_t$에는 4개 단어의 분산 처리 벡터가 순서대로 들어가게 됩니다. 펼친 구조로 표현하면 $x_0$와 $x_2$는 <em>no</em>의 분산 처리 벡터를, $x_1$와 $x_3$는 각각 <em>pain</em>과 <em>gain</em>의 분산 처리 벡터를 나타냅니다.</p>

<h3 id="h_t-t-시점의-은닉-상태">$h_t$: $t$ 시점의 은닉 상태</h3>

<p>$h_t$는 이전 단계의 은닉 상태인 $h_{t-1}$과 현 단계의 입력값인 $x_t$로 계산합니다. 즉, 이전 단계까지 누적 처리된 정보와 현 단계의 입력 정보를 합쳐서 현 단계까지 누적된 정보를 담게 되죠. 현재까지의 ‘상태’를 표현하기 때문에 <strong>은닉 상태(hidden state)</strong>라고 부릅니다. 그리고 처음부터 현재까지의 정보를 기억하는 것과 마찬가지이므로 모델의 <strong>메모리(memory)</strong>에 해당하기도 합니다.</p>

<p>Feed forward ANN에서 $x_0$, $x_1$, $x_2$의 순서를 바꿔도 출력되는 예측값은 그대로입니다. 그러나 값이 순차적으로 입력되는 RNN의 입력 $x_t$는, 어떤 값이 어떤 순서로 들어가느냐에 따라 출력되는 예측값이 달라집니다. $h_t$를 계산할 때 이전 데이터에 대한 정보가 반영되기 때문입니다.</p>

<h3 id="o_t-t-시점의-출력값">$o_t$: $t$ 시점의 출력값</h3>

<p>$o_t$는 $h_t$를 처리해서 출력하는 예측값입니다. 예를 들어 <em>no</em>에 대한 정보가 담긴 $h_0$와 <em>pain</em>의 분산 처리 벡터인 $x_1$의 정보를 합쳐 $h_1$를 계산해 낸 후, <em>no pain</em> 다음으로 나올 단어를 예측한 값이 $o_1$인 것이죠. $o_t$는 모든 RNN 모델에 필수적인 값은 아닙니다. 영화 리뷰에서 사람들의 반응을 긍정 혹은 부정으로 판단하고 싶을 때, 우리는 리뷰 속 단어의 다음에 나올 단어를 예측하기보다는 문장 전체에 대한 출력이 필요하기 때문입니다.</p>

<h2 id="rnn-vs-feed-forward-ann">RNN vs feed forward ANN</h2>

<p>Feed forward ANN과 RNN의 구조를 비교해 알 수 있는 RNN의 주요 특징은 다음과 같습니다.</p>

<p><br /></p>
<blockquote>
  <p>1) 직전 입력의 순차적 처리 정보가 반영된다.</p>
</blockquote>

<p>Feed forward ANN에서는 입력값의 순서가 예측값에 영향을 미치지 않았던 반면, RNN에서는 입력값이 들어오는 순서대로 은닉 상태가 업데이트되기 때문에, 순서가 있는 순차적 데이터를 처리하는 데에 적합합니다.</p>

<p><br /></p>
<blockquote>
  <p>2) 하나의 뉴런을 순환하므로 같은 가중치를 공유한다.</p>
</blockquote>

<p>ANN에서 우리는 신경망의 깊이가 깊어질수록 각 뉴런에 필요한 가중치들을 각각 고려해야 했습니다. 각 입력과 가중치 곱의 합을 수식 $\sum_{i=1}^{m}x_iw_i$로 표현했던 것처럼, 각 입력값에 대응하는 가중치가 여러 개 필요했었죠. 그러나 RNN은 하나의 뉴런을 반복해서 순환하기 때문에, 순차적으로 들어오는 입력값들은 동일한 가중치와 곱해집니다. 따라서 입력 데이터의 길이가 길어져 신경망의 깊이가 깊어지더라도 동일한 가중치를 사용하게 됩니다.</p>

<p><br /></p>
<blockquote>
  <p>3) 입력 길이가 가변적이다.</p>
</blockquote>

<p>ANN 모델을 구현할 때는 몇 개의 입력 노드를 사용할 건지 모델 초기화 단계에서 미리 지정해야 했습니다. 따라서 100개의 입력노드를 가진 ANN 모델은 100개의 입력값만 받을 수 있었죠. 하지만 RNN은 입력 길이가 길어져도 (즉, 문장의 길이가 길어져도) 모델을 구성하는 뉴런을 그대로 사용하고, 순환 횟수만 늘려주면 되기 때문에 가변적인 길이의 입력값을 넣을 수 있습니다.</p>

<h2 id="rnn의-학습">RNN의 학습</h2>

<p>RNN을 펼쳐보면 여러 뉴런이 겹쳐 있는 인공신경망과 유사합니다. 따라서 RNN도 ANN과 마찬가지로 순전파와 역전파를 반복하며 학습합니다. 다만, 순환 구조로 인해 학습 과정에서 차이가 생깁니다. 이번에는 RNN의 순전파와 역전파가 어떻게 진행되는지 살펴보겠습니다.</p>

<h3 id="순전파">순전파</h3>

<p>반복 순환되는 RNN의 뉴런 내부 과정을 살펴보면 feed forward ANN과 비슷합니다. 선형 변환과 비선형 변환을 차례대로 거치죠.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/RNN-순전파의-선형-비선형-변환.png" alt="RNN 순전파의 선형, 비선형 변환" width="" /></p>
<p class="img_caption">RNN 순전파의 선형, 비선형 변환</p>

<p>위 그림처럼 입력값($x_t$)과 직전 뉴런의 은닉상태($h_{t-1}$)가 입력되면 가중치와 편향을 계산해 선형 변환을 실시합니다.</p>

<p>$$Z = W_x x_t + W_h h_{t-1} + b$$</p>

<p>입력되는 $x_t$와 $h_{t-1}$에 해당하는 가중치를 각각 곱해주고 편향을 더해주는, feed forward ANN에서도 볼 수 있었던 선형 변환입니다. 여기서는 선형 변환된 값을 $Z$라고 하겠습니다.</p>

<p>이후 선형 변환된 값 $Z$에 비선형 변환 처리를 한 값이 $h_t$가 됩니다. 비선형 변환을 구현하기 위해 활성화 함수를 사용했던 것, 기억나시나요? 이번에 사용할 활성화 함수는 <strong>쌍곡선 함수</strong>라고도 불리는 <strong>hyperbolic tangent 함수(이하 tanh 함수)</strong>입니다.</p>

<p>$$h_t = \textrm{tanh}(Z)$$</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/tanh-함수와-sigmoid-함수.png" alt="tanh 함수와 sigmoid 함수" width="350" /></p>
<p class="img_caption">논문 &lt;<em>The Most Used Activation Functions: Classic Versus Current</em>&gt;</p>

<p>두 함수의 그래프를 비교해 보면 알 수 있듯, tanh 함수는 sigmoid 함수와 유사한 양상을 띠고 있습니다. 다만 sigmoid 함수의 출력값은 0과 1 사이인 반면, tanh 함수의 출력값은 -1과 1 사이입니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/tanh-함수와-sigmoid-함수의-도함수-비교.png" alt="tanh 함수와 sigmoid 함수의 도함수 비교" width="380" /></p>
<p class="img_caption">tanh 함수와 sigmoid 함수의 도함수 비교
    <a class="img_ref" href="https://www.baeldung.com/cs/sigmoid-vs-tanh-functions">(출처)</a>
</p>

<p>두 함수의 미분 그래프를 비교해 보면 tanh 함수가 가질 수 있는 기울기(미분값)의 폭이 더 넓습니다. sigmoid 함수의 미분 최댓값은 0.3, tanh 함수의 미분 최댓값은 1입니다. 덕분에 tanh 함수는 기울기 소실 증상이 더 적은 편입니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/RNN-순전파.png" alt="RNN의 순전파" width="" /></p>
<p class="img_caption">RNN의 순전파</p>

<p>펼친 RNN은 여러 뉴런이 겹쳐있는 인공 신경망으로 볼 수 있다고 했습니다. 따라서 인공 신경망의 학습과 같은 순서로 순전파를 먼저 수행해 임의의 가중치에 대한 예측값을 출력합니다.</p>

<h3 id="역전파-bpttbackpropagation-through-time">역전파, BPTT(Backpropagation Through Time)</h3>

<p>그런 다음, 정답과의 오차를 줄이기 위해 손실 함수로 계산한 오차를 역전파합니다. 이때, RNN에서의 역전파는 시간(혹은 순서)을 거슬러 가기 때문에 <strong>Backpropagation Through Time(BPTT)</strong>라고 부릅니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/RNN-순전파와-BPTT.png" alt="RNN의 순전파와 BPTT" width="" /></p>
<p class="img_caption">RNN의 순전파와 BPTT</p>

<p>Feed forward ANN에서도 그랬듯, 손실에 대한 가중치의 미분인 $\frac{dL}{dW_x}$와 $\frac{dL}{dW_h}$를 구할 때는 미분의 연쇄 법칙을 따릅니다. 순환 구조의 RNN에서 보면 다음과 같습니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/RNN에서-미분의-연쇄법칙.png" alt="RNN에서의 미분의 연쇄법칙" width="" /></p>
<p class="img_caption">RNN에서 미분의 연쇄법칙</p>

<p>다소 복잡해 보이지만, 미분의 연쇄법칙을 통해 결국 손실에 대한 $W_x$, $W_h$, $h_{t-1}$, $x_t$의 기울기를 모두 구할 수 있다는 것을 기억하세요.</p>

<h3 id="truncated-bptt">Truncated BPTT</h3>
<p>RNN에서는 역전파를 계산할 때, 중간에 계산된 값인 $h_t$를 보관해야 합니다. 이때, 입력되는 데이터가 너무 길어져 RNN의 깊이가 깊어지면, 보관해야 하는 $h_t$의 수가 많아집니다. 데이터를 보관하고 연산 처리하기 위한 컴퓨팅 자원이 증가하게 되죠. 뿐만아니라 미분의 연쇄법칙으로 인해 기울기가 누적되다 보니, 최종 출력에서 멀어질수록 계산된 기울기가 너무 작아져 버리는 기울기 소실 문제도 일어납니다. 이를 해결하기 위해 큰 시계열 데이터를 다루는 RNN은 적당한 길이로 잘라서 역전파를 수행하는데, 이를 <strong>Truncated BPTT</strong>라고 합니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/Truncated-BPTT.png" alt="Truncated BPTT" width="" /></p>
<p class="img_caption">Truncated BPTT</p>

<p>이때 중요한 것은, 신경망의 역전파만 자른다는 것입니다. 이렇게 하면 잘린 조각 내에서는 다른 조각들의 데이터를 반영하지 않고 오차와 기울기를 계산하기 때문에 기울기가 소실되거나, 누적된 이전 데이터를 보관할 메모리에 대한 걱정은 덜 수 있습니다.</p>

<h2 id="tensorflow로-rnn-구현하기">Tensorflow로 RNN 구현하기</h2>

<p>오늘 우리가 알아본 RNN은 tensorflow의 API로 간단하게 구현할 수 있습니다. 먼저 필요한 라이브러리를 import하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">SimpleRNN</span>
</code></pre></div></div>

<p>첫 번째로, RNN 모델을 학습시킬 데이터를 만듭니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>이번 예시에서는 1차원 벡터 5개로 구성된 입력 샘플 10개에 대해 0 또는 1로 이진 분류를 수행하려고 합니다.</p>

<p>따라서 입력 데이터인 <code class="language-plaintext highlighter-rouge">X</code>는 <code class="language-plaintext highlighter-rouge">(10, 5, 1)</code> 크기의 행렬로 준비하고, 정답인 <code class="language-plaintext highlighter-rouge">y</code>는 0 이상 2 미만의 정수로 <code class="language-plaintext highlighter-rouge">(10, 1)</code> 크기의 행렬로 준비합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">SimpleRNN</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>
<p>두 번째로, tensorflow의 <code class="language-plaintext highlighter-rouge">SimpleRNN</code> API와 출력층이 되는 <code class="language-plaintext highlighter-rouge">Dense</code> API를 연결해 순차 모델인 RNN을 만듭니다.</p>

<p>이때, RNN 계층에서 은닉 상태 벡터는 8차원으로 임의 설정하고, 활성화 함수로 <code class="language-plaintext highlighter-rouge">tanh</code>를 사용했습니다. 출력층에서는 이진 분류를 위해 <code class="language-plaintext highlighter-rouge">sigmoid</code> 함수를 활성화 함수로 사용했습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">예측값:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>모델을 컴파일할 때 손실함수로 BCE(<code class="language-plaintext highlighter-rouge">binary_crossentropy</code>)를 사용했습니다.</p>

<p><code class="language-plaintext highlighter-rouge">.fit()</code>메소드로 모델을 학습시킬 때, <code class="language-plaintext highlighter-rouge">epochs</code>를 <code class="language-plaintext highlighter-rouge">20</code>으로 설정해 학습을 20회 반복하도록 합니다.</p>

<p>학습이 완료된 후 <code class="language-plaintext highlighter-rouge">.predict()</code>메소드로 <code class="language-plaintext highlighter-rouge">X</code>에 대한 모델의 예측값을 출력합니다.</p>

<h2 id="rnn의-한계">RNN의 한계</h2>

<p>Truncated BPTT로 역전파를 실행해도, 굉장히 긴 시계열 데이터가 입력되었을 때 RNN이 아주 오래 전의 정보는 제대로 기억하지 못한다는 문제는 여전합니다. Truncated BPTT는 계산 효율을 크게 증가시키지만 잘라진 구간 내부에서만 역전파를 수행하므로, 이전 구간에서의 정보를 충분히 반영하지 못할 가능성이 있습니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/RNN-장기-의존성-문제.gif" alt="RNN 장기 의존성 문제" width="" /></p>
<p class="img_caption">RNN 장기 의존성 문제
    <a class="img_ref" href="https://youtu.be/LHXXI4-IEns">(출처)</a>
</p>

<p>위 그림은 “What time is it?”이라는 문장의 각 단어를 순차적으로 입력할 때, RNN의 은닉상태가 어떻게 변화하는지를 보여주고 있습니다. 새로운 단어를 처리할 때마다, 이전의 은닉상태는 새로운 입력과 결합되면서 점점 희석됩니다. 처음 “What”이 입력되었을 때, 은닉 상태는 100% “What”에 대한 정보로 채워져 있지만, 이후 “time”, “is”, “it”, “?” 순으로 단어가 입력될수록 은닉 상태에서 “What”의 정보는 점차 줄어듭니다. 최종적으로 “?“를 입력할 때는 “What”에 대한 정보가 거의 남아 있지 않습니다.</p>

<p>이는 ANN에서 발생했던 기울기 소실 문제(Vanishing Gradient Problem)가 RNN에서도 나타나면서 일어나는 일입니다. ANN에서 알아봤던 기울기 소실 문제를 기억하시나요? 출력층에서 입력층으로 손실이 역전파될수록 미분의 연쇄법칙에 의해 도함수가 누적되어 곱해집니다. 이때 활성화 함수의 미분값이 0~1 사이이기 때문에 도함수가 누적될수록 기울기는 0에 가까워집니다. 역전파되는 기울기로 가중치의 값을 업데이트해야 하는데, 기울기가 0에 가까우니 입력층에 가까울수록 가중치는 업데이트되지 않는 것이죠.</p>

<p>RNN에서도 마찬가지입니다. 순차 데이터에서 입력층에 가까운 데이터는 비교적 ‘과거’에 해당하는 정보일 것입니다. RNN의 가중치에 과거의 정보가 학습되지 않으니, 새로운 단어를 예측할 때 과거의 정보는 반영하기 어려워집니다.</p>

<p><img src="/assets/images/posts/2025-01-31-RNN/장기-의존성-문제-예문.png" alt="장기 의존성 문제 예문" width="400" /></p>
<p class="img_caption">장기 의존성 문제 예문</p>

<p>위 문장에서 파란 빈칸에 들어갈 단어를 <span style="color:#1c70ff">Korean</span>으로 예측하려면, 문장 초반에 등장한 <span style="color:#ff6c00">Korea</span>라는 정보가 반영되어야 합니다. 하지만 문장이 길어질수록 <span style="color:#ff6c00">Korea</span>와 파란 빈칸 사이의 거리가 멀어지고, RNN은 <span style="color:#ff6c00">Korea</span>를 제대로 기억하지 못하게 됩니다. 이처럼 RNN이 과거의 정보를 충분히 학습하지 못하는 현상을 <strong>장기 의존성 문제(Long-Term Dependency)</strong>라고 합니다.</p>

<p>이러한 한계로 RNN은 먼 과거의 정보가 있어야 하는 문장 이해, 문맥 유지 등의 작업에서 한계를 가지게 됩니다. 이를 극복하기 위해 LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)가 등장했습니다. 다음 글에서는 LSTM과 GRU가 어떻게 RNN의 구조를 개선하여 장기 의존성 문제를 해결하는지, 그 작동 원리와 장점을 알아보고 RNN과의 차이점을 살펴보겠습니다.</p>]]></content><author><name>yunha</name></author><category term="Deep Learning 기초" /><category term="deep-learning" /><category term="RNN" /><category term="순환신경망" /><category term="BPTT" /><category term="Truncated-BPTT" /><summary type="html"><![CDATA[앞선 글에서 알아본 전통적인 인공 신경망은 데이터 속 패턴을 학습해 새로운 값을 예측해 낼 수 있습니다. 순전파를 통해 임의의 가중치로 모델의 예측값을 계산해 내고, 역전파를 통해 예측값과 정답 사이의 오차를 줄이는 방향으로 가중치를 업데이트했습니다. 이 모든 과정이 데이터의 패턴을 가장 잘 나타내는 가중치를 찾아내는 학습이었죠. 하지만 이 전통적인 인공 신경망은 순서가 있는 순차 데이터(sequential data, 이하 시퀀스 혹은 시퀀스 데이터)를 학습하기에 적합하지 않습니다. 이번 글에서는 이 시퀀스를 다루는 시퀀스 모델 중 가장 기본적인 인공신경망 모델인 순환 신경망(Recurrent Neural Network, 이하 RNN)을 전통적인 ANN과 비교하며 알아보고, tensorflow로 간단히 구현한 후, 어떤 한계가 남아있는지 소개하겠습니다.]]></summary></entry><entry><title type="html">ANN의 발전과 구조</title><link href="https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2024/12/09/ANN-1.html" rel="alternate" type="text/html" title="ANN의 발전과 구조" /><published>2024-12-09T00:00:00+00:00</published><updated>2024-12-09T00:00:00+00:00</updated><id>https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2024/12/09/ANN-1</id><content type="html" xml:base="https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2024/12/09/ANN-1.html"><![CDATA[<p><img src="/assets/images/posts/2024-12-09-ANN-1/앨런튜링-can-machines-think.png" alt="앨런 튜링 - Can machines think?" /></p>
<p class="img_caption">앨런 튜링의 논문, &lt;<em>Computer Machinery and Intelligence</em>&gt;의 일부</p>

<p>인간은 오랜 세월 동안 기계가 인간처럼 사고할 수 있을지 호기심을 가져왔습니다. 1950년, 엘런 튜링(Alan Turing)은 자신의 논문 &lt;<em>Computer Machinery and Intelligence</em>&gt;에서 “Can machines think?”라는 질문을 던지며, 기계의 지능에 대한 논의를 본격적으로 시작했습니다.</p>

<p>그는 이 질문에 답하기 위해 인간과 기계의 대화를 통해 기계의 지능을 평가하는 튜링 테스트를 제안했는데, 이러한 호기심과 도전은 인간의 지능을 모방하고 이를 넘어서는 시스템을 설계하기 위한 다양한 시도로 이어졌습니다.</p>

<p>특히 <strong>인공신경망(Artificial Neural Network, ANN)</strong>은 생물학적 뉴런의 작동 방식을 본뜬 알고리즘으로, 기계가 데이터를 학습하고 패턴을 인식하는 방식을 혁신적으로 변화시켰습니다. ANN의 발전은 초기의 단순한 뉴런 모델에서 시작해 오늘날의 딥러닝으로 이어지며, 기계 지능의 가능성을 증명하는 데 큰 역할을 해왔습니다. 오늘은 이러한 신경망의 역사를 통해 인공신경망이 어떻게 발전해 왔는지 살펴보고자 합니다.</p>

<h2 id="인간의-뉴런을-기계적으로-구현한-첫-모델-m-p-뉴런">인간의 뉴런을 기계적으로 구현한 첫 모델, M-P 뉴런</h2>

<p>인간의 지능을 모방하기 위한 시도는, 인간의 생물학적인 뉴런을 모방한 논리적 모델로부터 시작했습니다. 1943년, 워렌 맥컬록(Warren McCulloch)과 월터 피츠(Walter Pitts)는 &lt;<em>A Logical Calculus of Ideas Immanent in Nervous Activity</em>&gt;라는 논문을 발표하고, 뉴런의 동작을 이진 논리로 설명하는 M-P 뉴런을 소개했습니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/생물학적-뉴런.png" alt="생물학적 뉴런의 구조" width="400" /></p>
<p class="img_caption">생물학적 뉴런의 구조
    <a class="img_ref" href="https://namu.wiki/w/뉴런">(출처)</a>    
</p>

<p>인간의 뇌는 수백억 개의 뉴런으로 이루어져 있고, 각 뉴런은 신호를 처리한 뒤 다른 뉴런으로 신호를 전달합니다. 뉴런의 수상돌기에서 신호를 받아들이면, 신경세포체에서 받은 신호를 종합하고 처리하죠. 계산 결과가 특정 임곗값을 초과하면 축삭 말단 부근의 시냅스를 통해 다음 뉴런으로 신호가 전달되며, 뇌 전체에서 이 과정을 반복해 정보를 처리합니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/M-P-뉴런.png" alt="M-P 뉴런" /></p>
<p class="img_caption">M-P 뉴런
    <a class="img_ref" href="https://medium.com/@siddharthshah2601/mcculloch-pitts-neuron-a-computational-model-of-biological-neuron-ce57239a951e">(출처)</a>
</p>

<p>앞서 살펴본 생물학적인 뉴런의 동작을 단순화한 수학적 모델인 M-P 뉴런은 위와 같이 그릴 수 있습니다. 인간의 뉴런이 수상돌기에서 여러 입력 신호를 받는 것처럼, M-P 뉴런에도 2진 값을 가진 여러 개의 입력값($x_1$, $x_2$, …, $x_m$)이 전달됩니다. 그런 다음, 신경세포체에서 신호를 종합하고 다음 뉴런으로의 신호 전달 여부를 결정할 때 임계값이 필요했듯이, M-P 뉴런에서도 입력값의 합이 특정 임계값을 초과하면 뉴런이 활성화되어 1이, 그렇지 않으면 뉴런이 비활성화 되어 0이 출력됩니다. 임계값을 조정함으로써 기본적인 논리 회로(AND, OR, NOT)를 구현할 수 있고, 여러 개의 뉴런을 계층적으로 결합함으로써 조금 더 복잡한 계산도 가능해집니다.</p>

<p>이어 1949년, 캐나다의 심리학자 도널드 올딩 헵(Donald Olding Hebb)은 ‘헵의 이론(Hebbian theory)’을 발표하며 동시에 활성화되는 뉴런들은 함께 연결되고 함께 강화한다고 제안했습니다. 활동 중 함께 발화(활성화)하는 뉴런들은 강해진다는 생물학적 법칙에서 출발한 헵의 이론은 이후에 가중치를 활용한 신경망 학습 알고리즘의 기반이 됩니다.</p>

<h2 id="인공지능의-본격적인-등장-퍼셉트론">인공지능의 본격적인 등장, 퍼셉트론</h2>

<p>인공지능의 선구자로 불리는 미국의 전산학자 존 매카시(John McCarthy)는 1956년, 다트머스 대학 학회(Dartmouth Conference)를 개최하고 ‘인공지능’이라는 용어를 처음으로 사용했습니다. 인공지능(Artificial Intelligence, AI)을 ‘고도의 지능을 가진 컴퓨터 디바이스를 만들기 위한 과학과 공학’으로 정의했는데, 이때를 시작으로 관련 연구들은 ‘인공지능’이라는 이름을 내걸고 더욱 활발해졌습니다. 머지않아 1958년, 최초의 인공 신경망인 퍼셉트론(Perceptron)이 등장합니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/perceptrons.png" alt="퍼셉트론" /></p>
<p class="img_caption">퍼셉트론
    <a class="img_ref" href="https://pub.aimind.so/perceptron-101-the-building-blocks-of-a-neural-network-496f6b9b3826">(출처)</a>
</p>

<p>미국의 심리학자 프랑크 로젠블랫(Frank Rosenblatt)은 맥컬록-피츠의 모델과 헵의 이론에서 영감을 얻어 가중치와 학습의 개념을 추가한 퍼셉트론을 개발했습니다. 입력값마다 각각 입력의 중요도를 나타내는 가중치를 곱한 후 모두 합해 활성화 함수에 전달합니다. 활성화 함수에서 출력되는 결과를 보고 앞서 곱해진 가중치(weights)를 조정해 출력된 예측값과 정답의 오차를 줄이는 학습 알고리즘이 포함된 것이죠.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/계단함수.png" alt="계단 함수" width="400" /></p>
<p class="img_caption">계단 함수
	<a class="img_ref" href="https://pub.aimind.so/perceptron-101-the-building-blocks-of-a-neural-network-496f6b9b3826">(출처)</a>
</p>

<p>초기 퍼셉트론에서 사용된 활성화 함수는 위 그래프처럼 임곗값(threshold)을 기준으로 출력이 0 혹은 1로 바뀌는 계단 함수(step function)였습니다. 활성화 함수는 이처럼 가중치가 반영된 입력값의 총합이 뉴런의 활성화를 일으키는지를 결정하는 역할을 합니다. 현대에 들어서는 더 다양한 활성화 함수들이 있는데, 이 부분은 뒤에서 인공신경망의 학습을 다루면서 더 살펴보겠습니다.</p>

<h2 id="퍼셉트론이-해결하지-못한-문제-xor">퍼셉트론이 해결하지 못한 문제, XOR</h2>

<p>퍼셉트론이 등장하면서 인공지능에 대한 연구는 부흥기를 맞이했지만, 그 기간은 그리 길게 이어지지 못했습니다. 1969년, 미국의 컴퓨터 과학자 마빈 리 민스키(Marvin Lee Minsky)가 저서 &lt;<em>Perceptrons</em>&gt;에서 퍼셉트론은 XOR 문제와 같은 비선형 문제를 해결할 수 없다는 한계를 지적했기 때문입니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/ANN-diagrams_퍼셉트론의%20층.png" alt="퍼셉트론의 층" width="500" /></p>
<p class="img_caption">퍼셉트론의 층</p>

<p>초기의 퍼셉트론은 값을 입력받는 입력층과, 출력값을 결정하는 출력층까지 총 두 단계로만 이루어진 단층 퍼셉트론이었습니다. 이러한 단층 구조는 입력값을 선형 결합해서 단일한 출력만을 낼 수 있기 때문에, 비선형 경계를 표현하는 데에 한계가 있습니다.</p>

<p>대표적인 비선형 문제 중 하나인 XOR 회로를 그림으로 나타내면 아래와 같습니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/논리회로.png" alt="논리회로" /></p>
<p class="img_caption">논리 회로
    <a class="img_ref" href="https://pyimagesearch.com/2021/05/06/implementing-the-perceptron-neural-network-with-python/">(출처)</a>
</p>

<p>다른 논리 회로인 AND 회로와 OR 회로는 선형적인 경계로 데이터를 분리할 수 있지만, XOR 회로에서는 데이터를 표현할 하나의 선형 경계를 그릴 수 없습니다. 단층 구조라서 선형 경계만을 그릴 수 있는 퍼셉트론의 한계가 여기서 나타난 것이죠. 이후 인공 신경망 연구에는 오랜 침체기가 찾아왔습니다.</p>

<h2 id="다층-신경망과-역전파-알고리즘">다층 신경망과 역전파 알고리즘</h2>

<p>퍼셉트론이 단층 신경망이라서 위와 같은 문제를 갖는 것이라면, 여러 층을 갖는 퍼셉트론은 이 문제를 해결할 수 있을까요?</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/ANN-diagrams_다층신경망.png" alt="다층신경망" /></p>
<p class="img_caption">다층신경망</p>

<p>다층 신경망은 단층 신경망에 하나 이상의 은닉층이 추가된 인공 신경망입니다. 은닉층이 2개 이상이면 심층 신경망(Deep Neural Network, DNN)이라고도 합니다. 각 층은 그림에서 동그라미로 표현된 노드로 구성되어 있습니다. 여기서 노드(node)는 앞서 계속 다뤄온 인공 뉴런을 의미합니다. 앞으로 인공 신경망을 구성하고 있는 인공 뉴런을 노드라고 표기하겠습니다. 입력층의 노드에는 입력 데이터가 들어있지만, 나머지 층의 노드는 각자 정해진 역할을 수행합니다. 그 역할의 내용은 다음과 같습니다.</p>

<ul>
  <li><strong>입력층 (Input Layer)</strong>: 입력 데이터의 전달</li>
  <li><strong>은닉층 (Hidden Layers)</strong>:
    <ul>
      <li>입력층에서 입력 데이터를 전달받거나 직전 은닉층의 계산 결과를 전달받음</li>
      <li>전달받은 값에 각각 가중치를 곱해 합산하고, 활성화 함수를 통해 다음 노드에 전달할 값 결정</li>
    </ul>
  </li>
  <li><strong>출력층 (Output Layer)</strong>: 신경망의 최종 출력 계산</li>
</ul>

<p>오랜 시간 동안 많은 연구자들이 노력했지만, 다층 신경망을 구현하는 것은 어려웠습니다. 은닉층에서 도출되는 각 노드의 출력값에는 오차를 측정할 정답이 없어, 은닉층의 가중치를 조정할 기준이 없었기 때문입니다. 그러나 1986년, 영국의 컴퓨터 과학자 제프리 힌튼(Geoffrey Hinton)이 다른 연구자들과 함께 다층 신경망을 학습시킬 수 있는 오류 역전파 알고리즘을 고안해 냈습니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/ANN-diagrams_오류%20역전파%20알고리즘.png" alt="오류 역전파 알고리즘" /></p>
<p class="img_caption">오류 역전파 알고리즘</p>

<p>정답이 있는 출력층에서 오차를 계산해 낼 수 있고, 이 오차(오류)를 출력층에서 입력층 방향으로 역으로 전파시키면서 은닉층의 가중치들을 재조정할 수 있게 된 것입니다. (역전파 알고리즘에 대해서는 이후 <a href="https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2024/12/09/ANN-2.html">ANN의 학습</a>에서 더 자세히 알아보겠습니다.) 오류 역전파 알고리즘으로 다층 신경망을 구현해 내면서, 이후에는 순환 신경망(RNN)과 합성곱 신경망(CNN)과 같은 구조들이 등장하여 시계열 데이터와 이미지 데이터를 처리할 수 있게 되었습니다.</p>

<h2 id="현대의-인공-신경망-은닉층에서의-동작">현대의 인공 신경망, 은닉층에서의 동작</h2>

<p>다층 신경망은 현대 인공 신경망의 기본 구조로 자리 잡았습니다. 이 현대 인공 신경망의 중요한 구성 요소 중 하나인 은닉층에 대해 조금 더 자세히 알아보겠습니다. 은닉층의 각 노드에서는 다음의 과정들이 순서대로 진행됩니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/ANN-diagrams_은닉층%20동작.png" alt="은닉층 동작" /></p>
<p class="img_caption">은닉층 동작</p>

<ol>
  <li>입력 데이터와 가중치(Weights)의 곱 계산</li>
  <li>계산한 곱과 편향(Bias)을 더함</li>
  <li>활성화 함수(Activation Function) 적용 결과 출력</li>
</ol>

<p>하나씩 살펴보겠습니다.</p>

<blockquote>
  <p>1) 입력 데이터와 가중치(Weights)의 곱 계산</p>
</blockquote>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/ANN-diagrams_은닉층%20동작%201.png" alt="은닉층 동작 1" /></p>
<p class="img_caption">은닉층 동작 1</p>

<p>노드로 입력 데이터들이 전달되면, 각 입력값에 맞는 가중치가 곱해집니다. 가중치는 각 입력값에 대한 중요도를 나타내는 숫자입니다. 따라서 입력값에 가중치를 곱함으로써 중요한 값은 비중을 키우고, 그렇지 않은 값은 비중을 줄일 수 있습니다.</p>

<blockquote>
  <p>2) 계산한 곱과 편향(Bias)을 더함</p>
</blockquote>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/ANN-diagrams_은닉층%20동작%202.png" alt="은닉층 동작 2" /></p>
<p class="img_caption">은닉층 동작 2</p>

<p>앞서 계산한 곱에 편향을 더해, 활성화 함수에 전달할 값을 결정하는 선형 변환 식 ($z=w_1x_1+w_2x_2+…+w_mx_m+b$)을 완성합니다. 입력값과 가중치의 곱에 편향을 더함으로써, 입력값이 0일 때도 유의미한 출력을 만들어낼 수 있도록 값을 조정합니다.</p>

<blockquote>
  <p>3) 활성화 함수(Activation Function) 적용 결과 출력</p>
</blockquote>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/ANN-diagrams_은닉층%20동작%203.png" alt="은닉층 동작 3" /></p>
<p class="img_caption">은닉층 동작 3</p>

<p>선형 변환된 값을 해당 노드의 출력값으로 변환하기 위해 활성화 함수를 사용합니다. 활성화 함수를 사용하는 주된 목적은 출력값에 비선형성을 추가하기 위함입니다. 선형 변환만을 사용한 신경망은 직선으로 데이터를 표현할 수 있는 경우에만 한정되어 있습니다. 선형 변환을 아무리 많이 결합해 봤자, 여러 층의 선형 변환은 결국 하나의 선형 모델과 동일한 결과를 내기 때문입니다. 하지만 복잡한 문제는 직선이 아닌, 보다 복잡한 모양의 경계(곡선, 면 등)가 필요한 경우가 많습니다. 활성화 함수는 이러한 비선형 경계를 만들 수 있도록 신경망에 비선형성을 도입합니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/ANN-diagrams_활성화%20함수의%20비선형성.png" alt="활성화 함수의 비선형성" /></p>
<p class="img_caption">활성화 함수의 비선형성</p>

<p>대표적인 활성화 함수로는 시그모이드 함수가 있습니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-1/시그모이드%20함수.png" alt="시그모이드 함수" width="500" /></p>
<p class="img_caption">시그모이드 함수
    <a class="img_ref" href="https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e">(출처)</a>
</p>

<p>시그모이드 함수는 실수 전체의 입력값에 대해 출력값이 0과 1 사이로 한정되어 있기 때문에, 특정 입력이 참인지 거짓인지를 분류하는 이진 분류(Binary Classification) 문제에서 사용됩니다. 시그모이드 함수의 출력값이 참 혹은 거짓 중 하나의 클래스에 속할 확률로 이해할 수 있기 때문입니다.</p>

<h2 id="마무리">마무리</h2>
<p>인공신경망(ANN)의 발전 과정은 인간의 지능을 모방하려는 도전에서 시작해, 학습 가능성과 문제 해결 능력을 점차 확장해 온 여정이었습니다. M-P 뉴런과 퍼셉트론은 초기의 중요한 기반을 마련했지만, XOR 문제와 같은 비선형 문제를 해결하지 못하며 한계를 드러냈습니다. 이러한 한계를 극복하기 위해 다층 신경망과 오류 역전파 알고리즘이 도입되었고, 이는 현대 딥러닝 기술의 핵심이 되었습니다. 특히 은닉층은 신경망이 복잡한 패턴을 학습하고 강력한 성능을 발휘할 수 있는 중요한 역할을 합니다.</p>

<p>이어지는 글에서는 ANN의 구조적 특징을 바탕으로 학습 과정과 그 원리에 대해 더 깊이 알아보겠습니다.</p>]]></content><author><name>yunha</name></author><category term="Deep Learning 기초" /><category term="deep-learning" /><category term="ANN" /><category term="인공신경망" /><category term="퍼셉트론" /><summary type="html"><![CDATA[앨런 튜링의 논문, &lt;Computer Machinery and Intelligence&gt;의 일부]]></summary></entry><entry><title type="html">ANN의 학습</title><link href="https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2024/12/09/ANN-2.html" rel="alternate" type="text/html" title="ANN의 학습" /><published>2024-12-09T00:00:00+00:00</published><updated>2024-12-09T00:00:00+00:00</updated><id>https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2024/12/09/ANN-2</id><content type="html" xml:base="https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2024/12/09/ANN-2.html"><![CDATA[<p>인공신경망(Artificial Neural Network, ANN)은 데이터를 학습하고 패턴을 인식하며 새로운 데이터를 예측하는 혁신적인 알고리즘입니다. <a href="https://woka-inc.github.io/deep%20learning%20%EA%B8%B0%EC%B4%88/2024/12/09/ANN-1.html">앞선 글</a>에서는 인공신경망의 역사와 개념을 살펴보며, 신경망이 어떻게 발전해 왔는지 이해했습니다.</p>

<p>이번 글에서는 ANN의 학습 과정을 자세히 탐구합니다. 임의의 가중치로 패턴의 예측값을 찾아내는 <strong>순전파(Forward Propagation)</strong>와 손실을 줄이기 위해 가중치를 업데이트하는 <strong>역전파(Backpropagation)</strong>의 원리를 설명하며, 이를 실제로 구현해볼 것입니다. 학생들의 학교 생활 데이터를 활용해 대입 합격 여부를 예측하는 신경망을 구축하면서, ANN의 작동 원리를 직접 확인할 수 있습니다.</p>

<p>이 글을 통해 ANN이 데이터를 학습하고 예측값을 도출하는 과정뿐만 아니라, 학습의 각 단계에서 발생하는 문제를 이해하고 이를 해결하는 방법까지 배우게 될 것입니다. 또한, 마지막에는 TensorFlow를 활용해 인공신경망 모델을 보다 간단하고 효율적으로 구축하는 방법도 소개합니다. ANN의 실질적인 활용과 구현에 대한 통찰을 얻어갈 수 있는 시간이 되길 바랍니다.</p>

<h2 id="학생의-대입-합격-여부-데이터셋">학생의 대입 합격 여부 데이터셋</h2>

<p>이번 글에서는 인공신경망의 학습 과정을 설명하면서, ‘학생의 학교생활 데이터를 입력받아 대입 합격 여부를 예측하는 인공신경망’을 함께 구현해 보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">absences</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>  <span class="c1"># 결석 일수
</span>    <span class="sh">"</span><span class="s">night_study</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># 야간자율학습 참여 여부
</span>    <span class="sh">"</span><span class="s">club_activity</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># 전공 관련 동아리 활동 여부
</span>    <span class="sh">"</span><span class="s">essay_length</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">532</span><span class="p">,</span> <span class="mi">427</span><span class="p">,</span> <span class="mi">735</span><span class="p">,</span> <span class="mi">635</span><span class="p">,</span> <span class="mi">142</span><span class="p">,</span> <span class="mi">395</span><span class="p">,</span> <span class="mi">439</span><span class="p">,</span> <span class="mi">519</span><span class="p">,</span> <span class="mi">498</span><span class="p">,</span> <span class="mi">425</span><span class="p">],</span> <span class="c1"># 자소서 글자 수
</span>    <span class="sh">"</span><span class="s">exam_score</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mf">98.5</span><span class="p">,</span> <span class="mf">89.0</span><span class="p">,</span> <span class="mf">72.6</span><span class="p">,</span> <span class="mf">87.3</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">96.2</span><span class="p">,</span> <span class="mf">92.8</span><span class="p">,</span> <span class="mf">97.2</span><span class="p">,</span> <span class="mi">71</span><span class="p">,</span> <span class="mf">86.2</span><span class="p">],</span> <span class="c1">#시험 점수
</span>    <span class="sh">"</span><span class="s">admission</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># 대입 합격 여부
</span><span class="p">}</span>

<span class="c1"># 데이터프레임으로 변환
</span><span class="n">df</span><span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">absences</th>
      <th style="text-align: right">night_study</th>
      <th style="text-align: right">club_activity</th>
      <th style="text-align: right">essay_length</th>
      <th style="text-align: right">exam_score</th>
      <th style="text-align: right">admission</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">532</td>
      <td style="text-align: right">98.5</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">427</td>
      <td style="text-align: right">89</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">735</td>
      <td style="text-align: right">72.6</td>
      <td style="text-align: right">0</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">635</td>
      <td style="text-align: right">87.3</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">142</td>
      <td style="text-align: right">85</td>
      <td style="text-align: right">0</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">395</td>
      <td style="text-align: right">96.2</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">439</td>
      <td style="text-align: right">92.8</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">519</td>
      <td style="text-align: right">97.2</td>
      <td style="text-align: right">0</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">498</td>
      <td style="text-align: right">71</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">425</td>
      <td style="text-align: right">86.2</td>
      <td style="text-align: right">0</td>
    </tr>
  </tbody>
</table>

<p>사용할 데이터 셋은 아래와 같은 입력 데이터를 포함하고 있습니다:</p>

<ul>
  <li>absences: 학교 결석일 수</li>
  <li>night_study: 야간자율학습 참여 여부 (0: 미참여, 1: 참여)</li>
  <li>club_activity: 전공 관련 동아리 활동 여부 (0: 미활동, 1: 활동)</li>
  <li>essay_length: 자기소개서 글자 수</li>
  <li>exam_score: 대입 시험 평균 점수</li>
</ul>

<p>마지막 항목인 admission은 대입 합격 여부로, 이 인공신경망이 예측해야 하는 목푯값(Target)입니다.</p>

<p>이 데이터에서 대학 합격 여부를 0 또는 1로 판단해야 하므로, 이 문제는 이진 분류(Binary Classification) 문제로 분류됩니다.</p>

<h2 id="인공신경망ann의-학습-목표">인공신경망(ANN)의 학습 목표</h2>

<p>인공 신경망(Artificial Neural Network, 이하 ANN)의 핵심 목표는 <strong>데이터에서 패턴을 학습하여 이를 하나의 선이나 면으로 표현하는 것</strong>입니다. 궁극적으로는 학습한 패턴을 활용해 새로운 데이터에 대해 <strong>정확한 예측</strong>을 수행해야 하죠. 이를 위해서는 주어진 학습용 데이터를 가지고 <strong>최적의 패턴</strong>을 학습해 내야 합니다. 여기서 패턴을 학습한다는 것은 <strong>최적의 가중치와 편향을 찾아내는 과정</strong>을 의미합니다.</p>

<p>ANN이 패턴을 학습하면, 학습한 패턴을 토대로 입력값에 대한 예측값을 도출할 수 있습니다. 이 예측값과 실제 정답(학습 데이터에서 입력값에 대응하는 출력값) 사이의 오차를 줄이면, 패턴이 더 정교해집니다. 오차는 은닉층 각 노드의 가중치와 편향 값을 조절하여 줄일 수 있습니다. 예를 들어, ‘아까는 $x_1$에 무게를 더 뒀다면, 이번엔 $x_2$에 무게를 더 둬 볼까?’라는 생각을 두 입력 데이터의 가중치와 편향 값을 조절함으로써 구현하는 거죠.</p>

<p>이렇게 예측값과 정답 간의 오차를 줄이는 것은 예측의 <strong>손실(Loss)</strong>을 최소화하는 것을 의미하며, 손실을 최소화하는 학습을 반복함으로써 ANN의 성능을 개선할 수 있습니다.</p>

<p>우리가 구현할 대입 합격 예측 인공신경망은 5가지 입력 데이터를 기반으로 정확한 대입 합격 여부를 예측할 수 있도록 학습되어야 합니다.</p>

<h2 id="순전파forward-propagation-임의의-가중치로-예측값-계산하기">순전파(Forward Propagation): 임의의 가중치로 예측값 계산하기</h2>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/ANN-diagrams_순전파.png" alt="순전파" width="500" /></p>
<p class="img_caption">순전파</p>

<p>손실을 줄이기 위해서는 정답과 비교할 <strong>예측값</strong>이 필요합니다. 이 예측값을 구하기 위해 앞서 알아봤던 신경망의 입력층, 은닉층, 출력층을 따라 입력 데이터를 ANN에 입력해 예측값을 도출합니다. 이 과정을 <strong>순전파(Forward Propagation)</strong>라고 합니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/ANN-diagrams_은닉층%201개%20인공신경망.png" alt="간단한 인공신경망" /></p>
<p class="img_caption">간단한 인공신경망</p>

<p>예시로 가져온 위 그림의 인공신경망은 은닉층이 1개인 매우 단순한 구조입니다. 입력층으로 전달되는 입력 데이터 $x$는 두 개의 피처 $x_1$과 $x_2$를 가지고 있습니다. 은닉층의 유일한 노드는 이 입력 데이터를 전달받아 가중치 $w_1$, $w_2$와 편향 $b$를 계산한 후, 계산 결과 $z$를 활성화 함수인 시그모이드 함수에 입력해 노드의 결괏값 $a$를 도출합니다. 은닉층에는 노드가 하나만 있으므로, 이 노드의 출력값 $a$는 출력층으로 전달되고 그대로 인공신경망 전체의 출력값인 예측값 $\hat{y}$로 반환됩니다.</p>

<p>여기서 중요한 점은 임의의 가중치와 편향을 가지고 값을 예측했다는 것입니다. 따라서 이후에는 ‘학습’을 통해 가중치와 편향을 최적의 값으로 조정하고, 신경망의 예측값 $\hat{y}$과 정답 $y$ 사이의 오차를 줄여 신경망의 예측 성능을 개선해야 합니다.</p>

<h3 id="잠깐-모든-입력-데이터에-대해-이-과정을-수행해야할텐데--벡터화">잠깐, 모든 입력 데이터에 대해 이 과정을 수행해야할텐데? → 벡터화</h3>
<p>학습 데이터는 여러 개의 입력데이터와 정답 쌍으로 구성되어 있습니다. 그렇다면 데이터 하나하나에 대해 위 과정을 반복해야 할까요? 코드로 구현한다면, 단순히 for 문을 사용하면 될까요?</p>

<p>Andrew Ng 교수님의 <a href="https://www.youtube.com/watch?v=ylkC33LWRjo">유명 강의</a>에서도 언급되었듯이, 단순한 연산을 여러 번 반복해야 할 때에는 for 문보다 더 효율적인 방법이 있습니다. 바로, 여러 데이터를 벡터화(Vectorization)하여 동시에 병렬 연산을 수행하는 것입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>

<span class="c1"># 벡터화한 뒤 연산 수행시간 측정
</span><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">벡터화한 뒤 연산: </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span> <span class="o">+</span> <span class="sh">"</span><span class="s">ms</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 반복문으로 연산 수행시간 측정
</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">):</span>
  <span class="n">c</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">반복문으로 연산: </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span> <span class="o">+</span> <span class="sh">"</span><span class="s">ms</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/assets/images/posts/2024-12-09-ANN-2/벡터화%20비교%20결과.png" alt="벡터화 결과 비교" width="400" /></p>
<p class="img_caption">벡터화 결과 비교</p>

<p>위 과정을 실행할 때마다 수행 시간이 조금씩 달라질 수 있지만, for 문을 활용한 연산이 벡터화를 통한 연산에 비해 몇십 배에서 몇백 배 더 오래 걸린다는 사실은 변함이 없습니다.</p>

<h3 id="대입-합격-예측-인공신경망의-첫-번째-순전파">대입 합격 예측 인공신경망의 첫 번째 순전파</h3>

<p>앞에서 배운 순전파 과정을 활용해 대입 합격 여부를 예측하는 인공신경망을 직접 구축해 보겠습니다.</p>

<p>먼저 데이터를 정리하겠습니다. 데이터 셋에서 독립변수 <code class="language-plaintext highlighter-rouge">X</code>(입력 데이터)와 종속변수 <code class="language-plaintext highlighter-rouge">y</code>(예측값이 가까워져야 할 정답)를 분리한 뒤, 입력 데이터를 정규화했습니다.</p>

<p><code class="language-plaintext highlighter-rouge">X</code>의 타입을 출력해 보니 numpy 배열임을 알 수 있습니다. 이는 여러 입력 데이터를 동시에 처리하기 위해 numpy 배열로 벡터화한 것입니다. <code class="language-plaintext highlighter-rouge">X</code>의 크기는 <code class="language-plaintext highlighter-rouge">10x5</code>로, 데이터 샘플 수(10)가 행, 입력 데이터의 종류 수(5)가 열로 나타난 것을 알 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 독립변수(X, 입력 데이터)와 종속변수(y, 정답) 분리
</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="sh">"</span><span class="s">absences</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">night_study</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">club_activity</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">essay_length</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">exam_score</span><span class="sh">"</span><span class="p">]].</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">admission</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># 데이터 정규화: 평균이 0, 표준편차가 1인 값으로 입력값의 스케일 조정
</span><span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">X</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">type</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/assets/images/posts/2024-12-09-ANN-2/독립변수%20출력%20결과.png" alt="독립변수 출력 결과" width="300" /></p>
<p class="img_caption">독립변수 출력 결과</p>

<p>이번 인공 신경망에서는 두 가지 활성화 함수를 사용했습니다. 시그모이드 함수와 함께 ReLU 함수도 사용했는데, 이 두 함수의 차이점에 대해서는 뒤에서 자세히 설명하겠습니다.</p>

<p>이 인공신경망은 입력층에 5개의 뉴런을 가지고 있습니다. 입력 데이터가 5개의 피처를 가지고 있기 때문입니다. 입력층 다음 순서로는 은닉층을 하나 거치게 되는데, 은닉층에는 임의로 3개의 뉴런을 설정했습니다. 출력층은 하나의 예측값(합격 여부 혹은 합격 확률)을 계산하므로 뉴런 1개가 필요합니다.</p>

<p>초기 가중치와 편향은 확실한 학습 결과를 보여주기 위해 랜덤 값으로 설정했습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 활성화 함수 정의
</span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># 초기 가중치와 편향 랜덤 초기화
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># 결과 재현성을 위한 랜덤 시드 설정
</span>
<span class="c1"># 입력층 -&gt; 은닉층
</span><span class="n">weights_input_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># (입력층 뉴런 5개 -&gt; 은닉층 뉴런 3개)
</span><span class="n">bias_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>           <span class="c1"># 은닉층 편향
</span>
<span class="c1"># 은닉층 -&gt; 출력층
</span><span class="n">weights_hidden_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (은닉층 뉴런 3개 -&gt; 출력 뉴런 1개)
</span><span class="n">bias_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>           <span class="c1"># 출력층 편향
</span></code></pre></div></div>

<p>이제 순전파 과정을 함수로 정의하고 실행해 보겠습니다.</p>

<p><code class="language-plaintext highlighter-rouge">forward_propagation()</code> 함수는 순전파 과정을 구현하며, 두 행렬(층에 입력된 값 행렬과 층의 가중치 행렬)을 곱하기 위해 <code class="language-plaintext highlighter-rouge">numpy.dot()</code> 메소드를 사용합니다. 그리고 가중치와 편향 값을 함수의 파라미터로 전달해, 이후에 업데이트된 가중치와 편향에 대해 재사용할 수 있도록 설계했습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 순전파 구현
</span><span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">):</span>
    <span class="c1"># 은닉층 계산
</span>    <span class="n">z_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights_input_hidden</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_hidden</span>
    <span class="n">a_hidden</span> <span class="o">=</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z_hidden</span><span class="p">)</span>  <span class="c1"># ReLU 활성화 함수 적용
</span>
    <span class="c1"># 출력층 계산
</span>    <span class="n">z_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">a_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_output</span>
    <span class="n">a_output</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z_output</span><span class="p">)</span>  <span class="c1"># Sigmoid 활성화 함수 적용
</span>
    <span class="k">return</span> <span class="n">a_output</span><span class="p">,</span> <span class="n">a_hidden</span>

<span class="c1"># 순전파 실행
</span><span class="n">output</span><span class="p">,</span> <span class="n">hidden_activation</span> <span class="o">=</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span>
                                                <span class="n">weights_input_hidden</span><span class="p">,</span>
                                                <span class="n">bias_hidden</span><span class="p">,</span>
                                                <span class="n">weights_hidden_output</span><span class="p">,</span>
                                                <span class="n">bias_output</span><span class="p">)</span>
</code></pre></div></div>

<p>결과는 아래와 같습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 결과 출력
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">입력 데이터 (정규화 후):</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">은닉층 활성화 값:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">hidden_activation</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">출력층 예측값 (합격 확률):</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/assets/images/posts/2024-12-09-ANN-2/순전파%20출력%20결과.png" alt="순전파 출력 결과" /></p>
<p class="img_caption">순전파 출력 결과</p>

<p>현재는 임의의 가중치와 편향 값을 사용했기 때문에, 예측값 또한 랜덤한 양상을 띠고 있습니다.</p>

<h2 id="손실함수-예측값과-정답-사이의-오차-계산">손실함수: 예측값과 정답 사이의 오차 계산</h2>

<p>순전파 과정에서 모델이 현재 패턴으로 예측값을 도출하면, 이 예측값과 정답 사이의 오차인 손실을 계산해야 합니다. 이를 기반으로 가중치를 조정하여 손실을 줄이는 방향으로 모델의 성능을 점진적으로 개선해 나가죠. 이때, 손실을 계산하는 데 사용되는 함수를 <strong>손실함수(Loss Function)</strong>라고 하며, 예측값($\hat{y}$)과 정답($y$) 사이의 손실을 $L(\hat{y}, y)$로 표현합니다.</p>

<p>어떤 데이터와 문제를 다루는지에 따라 다양한 손실 함수 중 적절한 함수를 선택해야 합니다. 대표적인 손실 함수 중 하나인 <strong>평균제곱오차(Mean Squared Error, MSE)</strong>는 말 그대로 예측값과 실제 값의 오차를 제곱한 뒤 평균을 내어 계산합니다. 따라서 결괏값이 작을수록 모델의 예측 오차가 작다는 것을 의미합니다.
$$ MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2$$</p>
<ul>
  <li>$y_i$: 실제값</li>
  <li>$\hat{y}_i$: 예측값</li>
  <li>$n$: 데이터의 총 개수</li>
</ul>

<p>MSE의 주요 특징은, 오차를 제곱해 양수로 만들기 때문에 데이터 간의 양수와 음수 오차가 상쇄되지 않는다는 점입니다. 또한, 오차를 제곱하기 때문에 큰 오차일수록 더 큰 영향을 미쳐 민감하게 반응하게 됩니다. MSE는 제곱 연산과 평균 연산으로 구성되어 있으며, 두 연산 모두 연속적이고 미분 가능하기 때문에 연속적인 값을 예측하는 회귀 문제에서 주로 사용됩니다.</p>

<h3 id="대입-합격-예측-인공신경망의-손실함수">대입 합격 예측 인공신경망의 손실함수</h3>

<p>대입 합격 여부를 예측하는 문제는 이진 분류 문제에 해당합니다. 이러한 이진 분류 문제를 다루는 인공신경망에는 손실함수로 Binary Cross-Entropy(이하 BCE)가 적합합니다.</p>

<p>앞선 순전파 과정에서 출력된 예측값은 0과 1 사이의 확률값을 나타내며, 이는 모델이 특정 클래스(합격)에 속할 가능성을 예측한 결과입니다. Cross-Entropy는 이러한 모델의 확률 출력값과 실제 정답 간의 차이를 측정하는 데에 최적화되어 있습니다. BCE는 실제 정답이 1일 때 예측 확률값이 1에 가까울수록, 실제 정답이 0일 때 예측 확률 값이 0에 가까울수록 손실 값을 낮게 계산합니다. 이러한 특징 때문에 BCE는 이진 분류 문제에서 널리 사용됩니다.</p>

<p>Binary Cross-Entropy 손실함수는 다음과 같이 정의됩니다.</p>

<p>$$BCE=-\frac{1}{n}\sum_{i=1}^{n}[y_i\log{\hat{y_i}}+(1-y_i)\log{(1-\hat{y_i})}]$$</p>
<ul>
  <li>$y_i$: 실제 클래스(0 또는 1)</li>
  <li>$\hat{y}_i$: 예측 확률값(0~1)</li>
  <li>$n$: 데이터의 총 개수</li>
</ul>

<p>Binary Cross-Entropy를 함수로 정의하고 이번 순전파 과정에서 계산된 예측값과 정답 사이의 손실을 계산해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Binary Cross-Entropy 손실 함수 정의
</span><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="c1"># y_true: 실제값 (0 또는 1)
</span>    <span class="c1"># y_pred: 예측 확률값 (0~1)
</span>
    <span class="c1"># 안정성을 위해 log의 입력값 범위를 제한
</span>    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="c1"># BCE 계산
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># BCE 손실 계산
</span><span class="n">loss</span> <span class="o">=</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Binary Cross-Entropy 손실 값:</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/assets/images/posts/2024-12-09-ANN-2/BCE%20출력%20결과.png" alt="BCE 출력 결과" width="500" /></p>
<p class="img_caption">BCE 출력 결과</p>

<p>BCE 손실 값은 0에 가까울수록 예측이 정확하다는 것을 의미합니다. 0.78이면 정답과 예측값 사이의 오차가 꽤 크다고 할 수 있겠죠? 이제 이 오차를 줄이기 위한 다음 단계를 살펴봅시다.</p>

<h2 id="경사하강법-손실함수의-최저점으로-다가가는-과정">경사하강법: 손실함수의 최저점으로 다가가는 과정</h2>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/ANN-diagrams_손실함수.png" alt="손실함수" width="500" /></p>
<p class="img_caption">손실함수</p>

<p>적합한 손실함수를 사용하면 손실을 그래프로 그렸을 때 위 그림처럼 손실이 최저가 되는 최적의 지점이 존재합니다. 우리의 목표이자 인공신경망(ANN)의 학습 목표는 바로 이 최저 손실 지점에 도달할 수 있는 (즉, 오차가 최저가 되는) 가중치를 찾아내는 것입니다. 학습 초기에는 가중치가 임의의 값으로 설정되었기 때문에, 단번에 최적의 지점을 찾기란 사실상 불가능합니다. 따라서 그림에서처럼 현재 가중치의 조합으로 계산한 손실 값을 확인한 뒤, 가중치를 수정해서 손실 함수를 따라 내리막 방향으로 한 걸음씩 이동해 나가는 과정을 반복해야 합니다. 이 과정을 <strong>경사하강법(Gradient Descent)</strong>라고 합니다.</p>

<p>경사하강법은 다음 두 단계로 이루어집니다:</p>
<ol>
  <li>가중치를 어느 방향으로, 얼만큼 움직일지 결정하기</li>
  <li>계산한 크기만큼 기존의 가중치 업데이트하기</li>
</ol>

<p><br /></p>
<blockquote>
  <p>1) 가중치를 어느 방향으로, 얼만큼 움직일지 결정하기</p>
</blockquote>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/ANN-diagrams_손실함수에서%20기울기.png" alt="손실함수에서 기울기" width="400" /></p>
<p class="img_caption">손실함수에서 기울기 따라 이동</p>

<p>현재 시작점에서 손실을 줄여 최적의 지점으로 다가가려면, 현재 지점에서의 <strong>기울기(Gradient)</strong>를 계산해 그 기울기에 -1을 곱한 방향으로 이동해야 합니다. 그리고 이 방향으로 얼마나 큰 한 걸음을 내딛을 것인지는 <strong>학습률(Learning Rate, α)</strong>로 결정합니다. 학습률은 경사하강법에서 각 단계의 이동 크기를 조절하는 중요한 하이퍼파라미터(딥러닝 모델에서 학습 과정에 영향을 미치는, 사용자가 직접 설정하는 값)입니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/ANN-diagrams_손실함수에서%20학습률.png" alt="손실함수에서 학습률" /></p>
<p class="img_caption">손실함수에서 학습률에 따른 이동 양상</p>

<p>학습률이 지나치게 크다면, 한 걸음이 너무 커져서 최적의 지점을 계속 지나쳐 손실 값이 발산할 위험이 있습니다. 반대로 학습률이 너무 작다면, 한 걸음씩 이동하는 속도가 너무 느려 전체 학습에 지나치게 많은 시간이 소요될 수 있습니다. 따라서 학습률을 반복적으로 실험하며 최적의 값을 찾아야 하며, 일반적으로 0.1에서 시작해 0.1배씩 줄여나가며 테스트합니다(<a href="https://modulabs.co.kr/blog/importance-of-learning-rate-and-finding-appropriate-learning-rate">출처</a>).</p>

<p>이렇게 한 걸음의 방향과 크기가 정해지면, 가중치가 한 번 움직이는(=업데이트되는) 값은 음의 기울기와 학습률의 곱인 $-α\frac{dL}{dw}$가 됩니다.</p>

<p><br /></p>
<blockquote>
  <p>2) 계산한 크기만큼 기존의 가중치 업데이트하기</p>
</blockquote>

<p>다음은 간단합니다. 기존의 가중치에 움직일 값을 더해 새로운 가중치로 업데이트하면 되는 것이죠. 이렇게 계산된 값만큼 가중치를 이동시키며 손실 값을 점진적으로 줄여갑니다.</p>

<p>경사하강법을 우리의 대입 합격 예측 인공신경망에 적용하기 위해서는, 가중치를 움직일 방향인 ‘-기울기(음의 기울기)’를 구할 수 있어야 합니다. 따라서 손실 함수에서의 도함수를 구하는 과정을 먼저 알아본 뒤, 우리 인공신경망에서 경사하강법을 구현해보겠습니다.</p>

<h2 id="역전파backpropagation-손실함수에-필요한-도함수를-구하는-과정">역전파(Backpropagation): 손실함수에 필요한 도함수를 구하는 과정</h2>

<p>ANN은 예측값과 정답 사이의 손실을 최소화하기 위해 가중치를 조정해서 경사하강법으로 최적의 가중치에 한 걸음씩 다가갑니다. 이를 위해 기울기의 반대 방향과 적절한 학습률을 곱해 가중치를 조정하는 과정이 필요합니다. 그런데 이 때, 기울기를 어떻게 계산할 수 있을까요?</p>

<p>기울기를 구하기 위해서는 손실 함수를 가중치에 대해 미분합니다. 하지만 복잡한 신경망의 경우 수십억 개의 가중치를 가지고 있기도 하며, 이 모든 가중치에 대해 일일이 미분하고 값을 수정하려면 계산량이 기하급수적으로 증가하게 됩니다. 처리 시간이 비현실적으로 길어져 비효율적이게 되죠.</p>

<p>이 문제를 해결하기 위해 딥러닝에서는 <strong>역전파(Backpropagation)</strong>라는 알고리즘을 사용합니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/ANN-diagrams_역전파.png" alt="역전파" width="500" /></p>
<p class="img_caption">역전파</p>

<p>앞서 살펴본 <strong>순전파(Forward Propagation)</strong>가 인공신경망의 <strong>입력층에서 출력층</strong>으로 계산을 진행하며 예측값을 구하는 과정이었다면, <strong>역전파(Backpropagation)</strong>는 <strong>출력층에서 입력층</strong>으로 계산을 진행합니다. 순전파의 목적이 주어진 입력값에 대해 신경망의 예측값을 계산하는 데 있었다면, 역전파의 목적은 <strong>손실함수의 가중치에 대한 도함수(기울기)</strong>를 구하고 이를 이용해 출력층에 가까운 은닉층부터 가중치를 업데이트하는 데 있습니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/ANN-diagrams_미분의%20연쇄법칙%20in%20역전파.png" alt="미분의 연쇄법칙" /></p>
<p class="img_caption">미분의 연쇄법칙</p>

<p>다소 복잡해보일 수 있지만, 신경망을 거슬러가는 역전파 과정의 빨간색 화살표는 모두 공통된 법칙을 사용하고 있습니다. 바로 <strong>미분의 연쇄법칙(chain rule)</strong>입니다. 순전파에서 예측값($\hat{y}$)을 출력한 후, 출력층에서 손실함수($L(\hat{y},y)$)를 구했었습니다. 이제는 반대 방향으로 이동하며 각 노드의 도함수를 구하고, 역전파의 이전 단계에서 계산된 도함수와의 연쇄법칙을 통해 손실함수에 대한 여러 변수의 도함수를 계산합니다.</p>

<p>위 그림의 신경망에서는 최종적으로 손실 함수에 대한 가중치 $w_1$과 $w_2$ 각각의 도함수($\frac{dL}{dw_1}$, $\frac{dL}{dw_2}$)를 구할 수 있습니다. 이렇게 계산된 도함수를 사용해 $w_1$과 $w_2$의 값을 업데이트합니다. 출력층 부근에서 계산한 오차가 입력층 부근의 가중치 $w_1$과 $w_2$까지 역전파되어 손실 함수에 대한 도함수를 구할 수 있게 된 것이죠.</p>

<p>위 그림에서는 은닉층이 한 층뿐이었지만, 여러 은닉층이 쌓여 있다면 출력층에서 입력층으로 향하는 역전파의 방향성 때문에 출력층에 가까운 은닉층의 가중치부터 수정됩니다. 입력층에 가까워질수록 누적되는 도함수가 많아지겠죠. 여기서, 활성화 함수인 시그모이드 함수로 인한 문제가 발생할 수 있습니다.</p>

<h3 id="활성화함수-제-2탄-어떤-활성화-함수를-선택할-것인가">활성화함수 제 2탄: 어떤 활성화 함수를 선택할 것인가?</h3>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/시그모이드와 도함수.png" alt="시그모이드와 도함수" width="500" /></p>
<p class="img_caption">시그모이드와 도함수
    <a class="img_ref" href="https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e">(출처)</a>
</p>

<p>파란색으로 그려진 시그모이드 함수를 보면, 입력값이 너무 크거나 작아질 경우 기울기(도함수)가 매우 작아지는 것을 볼 수 있습니다. 실제로 빨간 점선으로 그려진 시그모이드의 도함수를 보았을 때, 입력 값이 일정 수준을 넘어서면 도함수 값이 0에 가까워지는 경향을 보입니다.</p>

<p>역전파 과정에서 입력층으로 갈수록 기울기가 누적된다고 했는데, 극단적으로 모든 활성화 함수로 시그노이드 함수를 쓰게 되면 결국 누적된 도함수 값이 0에 매우 가까워지는 상황이 발생합니다. 이로 인해 발생하는 문제가 바로 <strong>기울기 소실 문제(Gradient Vanishing)</strong>입니다.</p>

<p>이 경우 층이 깊은 신경망에서 출력층 부근 은닉층의 가중치는 어느 정도 학습될 수 있지만, 입력층에 가까운 가중치들은 제대로 학습되기 어렵습니다. 따라서 상황에 따라 다른 활성화 함수를 채택해야 합니다.</p>

<p>그럼에도 불구하고 시그모이드 함수는 출력 값을 0~1의 범위로 제한한다는 특징 덕분에, 이진 분류를 위한 신경망의 출력층에서는 효과적으로 사용됩니다. 그 외의 은닉층에서는 시그노이드 함수 사용을 신중히 고려해야겠죠.</p>

<p><strong>ReLU 함수 (Rectified Linear Unit)</strong></p>

<p>시그모이드 함수의 기울기 소실 문제를 해결한 함수가 바로 ReLU(Rectified Linear Unit) 함수입니다. ReLU 함수는 입력값과 0 중 더 큰 값을 선택하는 방식으로 동작합니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/ReLU.png" alt="ReLU함수" width="350" /></p>
<p class="img_caption">ReLU 함수
    <a class="img_ref" href="https://limitsinx.tistory.com/40">(출처)</a>
</p>

<p>ReLU함수는 음수를 0으로 바꾸기 때문에 노드에 비선형성을 추가하는 기존 활성화 함수의 목적을 달성합니다. 동시에 양수의 입력값에 대해 기울기가 항상 1이기 때문에, 기울기 소실 문제에 대한 걱정이 필요 없죠.</p>

<p>다만, ReLU 함수의 음수의 입력값에 대한 기울기는 0이기 때문에, 음수의 입력값이 지속적으로 주어지면 해당 뉴런의 가중치는 학습되지 않는 죽은 뉴런 문제(Dying ReLU)가 발생하기도 합니다.</p>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/Leaky%20ReLU.png" alt="LeakyReLU함수" width="400" /></p>
<p class="img_caption">Leaky ReLU 함수
    <a class="img_ref" href="https://medium.com/@sreeku.ralla/activation-functions-relu-vs-leaky-relu-b8272dc0b1be">(출처)</a>
</p>

<p>이를 해결하기 위해 Leaky ReLU와 같은 변형된 활성화 함수를 사용할 수도 있습니다. Leaky ReLU는 음수의 입력값에 대해서도 약간의 기울기를 부여해, ReLU 함수의 죽은 뉴런 문제를 완화합니다.</p>

<p>하지만 Leaky ReLU는 추가적인 하이퍼파라미터(음수 기울기 값)를 설정해야 하며, 이 값의 최적화가 문제에 따라 달라질 수 있어 복잡성을 증가시킬 수 있습니다. 또한, ReLU는 구현이 간단하고 대부분의 경우 기본값으로도 충분히 좋은 성능을 보여주기 때문에 여전히 널리 사용됩니다.</p>

<h3 id="대입-합격-예측-인공신경망의-오차-역전파와-경사하강법">대입 합격 예측 인공신경망의 오차 역전파와 경사하강법</h3>

<p>경사하강법을 적용해 앞서 계산한 손실(약 0.783)을 줄여보도록 하겠습니다.</p>

<p>경사하강법을 적용하기 위해서는 두 가지 값이 필요했습니다. 손실값이 손실 함수의 경사를 따라 내려가도록 할 때, 한 걸음의 보폭이 되는 학습률과 한 걸음의 방향이 되는 기울기였습니다. 이번 실습에서는 학습률을 0.01로 설정했습니다.</p>

<p>기울기를 구하는 함수를 <code class="language-plaintext highlighter-rouge">backward_propagation()</code>으로 구현했습니다. 함수가 다소 복잡하지만, 각 가중치와 편향에 대한 기울기($d$~)가 <code class="language-plaintext highlighter-rouge">np.dot()</code> 연산을 통해 입력층에 다가갈수록 누적되고 있다는 것만 확인하고 넘어가도 좋습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 학습률 (learning rate)
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">def</span> <span class="nf">backward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">a_hidden</span><span class="p">,</span> <span class="n">a_output</span><span class="p">,</span> <span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">):</span>
    <span class="c1"># 출력층의 오차
</span>    <span class="n">d_output</span> <span class="o">=</span> <span class="n">a_output</span> <span class="o">-</span> <span class="n">y_true</span>  <span class="c1"># 출력층의 손실의 미분
</span>
    <span class="c1"># 은닉층 -&gt; 출력층의 가중치와 편향에 대한 기울기
</span>    <span class="n">d_weights_hidden_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">a_hidden</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_output</span><span class="p">)</span>  <span class="c1"># 은닉층 가중치 기울기
</span>    <span class="n">d_bias_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">d_output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># 출력층 편향 기울기
</span>
    <span class="c1"># 은닉층의 오차
</span>    <span class="n">d_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">d_output</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">a_hidden</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># ReLU의 미분 적용
</span>
    <span class="c1"># 입력층 -&gt; 은닉층의 가중치와 편향에 대한 기울기
</span>    <span class="n">d_weights_input_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">)</span>  <span class="c1"># 입력층 가중치 기울기
</span>    <span class="n">d_bias_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">d_hidden</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># 은닉층 편향 기울기
</span>
    <span class="k">return</span> <span class="n">d_weights_input_hidden</span><span class="p">,</span> <span class="n">d_bias_hidden</span><span class="p">,</span> <span class="n">d_weights_hidden_output</span><span class="p">,</span> <span class="n">d_bias_output</span>

<span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="c1"># weights: 기존 가중치
</span>    <span class="c1"># gradients: 가중치 기울기
</span>    <span class="c1"># learning_rate: 학습률
</span>    <span class="k">return</span> <span class="n">weights</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span>

<span class="c1"># 역전파를 통해 기울기 계산
</span><span class="n">d_weights_input_hidden</span><span class="p">,</span> <span class="n">d_bias_hidden</span><span class="p">,</span> <span class="n">d_weights_hidden_output</span><span class="p">,</span> <span class="n">d_bias_output</span> <span class="o">=</span> <span class="nf">backward_propagation</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">hidden_activation</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span>
<span class="p">)</span>
</code></pre></div></div>

<p>이렇게 구한 기울기를 가지고 입력층&amp;은닉층의 가중치/편향과 은닉층&amp;출력층의 가중치/편향 값을 업데이트 하겠습니다. 그 다음, 앞서 사용했던 순전파 함수를 재사용해 업데이트된 예측값을 계산해보았습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 가중치와 편향 업데이트
</span><span class="n">weights_input_hidden</span> <span class="o">=</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">d_weights_input_hidden</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">bias_hidden</span> <span class="o">=</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">bias_hidden</span><span class="p">,</span> <span class="n">d_bias_hidden</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">weights_hidden_output</span> <span class="o">=</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">d_weights_hidden_output</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">bias_output</span> <span class="o">=</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">bias_output</span><span class="p">,</span> <span class="n">d_bias_output</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># 순전파 재실행 (업데이트된 가중치 사용)
</span><span class="n">updated_output</span><span class="p">,</span> <span class="n">updated_hidden_activation</span> <span class="o">=</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">업데이트된 예측값 (합격 확률):</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">updated_output</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/예측값%20업데이트%20출력.png" alt="예측값 업데이트 출력" width="300" /></p>
<p class="img_caption">예측값 업데이트 출력</p>

<p>각 입력 데이터에 대해 도출한 예측값만 봐서는 앞선 순전파 결과와 크게 달라진 점이 눈에 띄지 않습니다. 예측값과 정답 사이의 오차를 계산하는 손실 함수를 적용해보면, 예측의 정확도가 어떻게 변화하는지 확인할 수 있겠죠?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 업데이트된 예측값의 손실
</span><span class="n">updated_loss</span> <span class="o">=</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">updated_output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">초기 Binary Cross-Entropy 손실 값: </span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">업데이트된 Binary Cross-Entropy 손실 값:</span><span class="sh">"</span><span class="p">,</span> <span class="n">updated_loss</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/손실%20비교.png" alt="손실 비교" width="700" /></p>
<p class="img_caption">손실 비교</p>

<p>손실이 줄었습니다! 미묘한 값이지만, 우리는 인공신경망에게 1회의 학습을 진행했습니다. 이 과정을 여러번 거듭할수록 손실이 줄고, 우리 인공신경망은 패턴을 더 잘 학습해서 더 나은 예측을 할 수 있습니다.</p>

<p>앞의 과정을 10번 더 반복해보고 손실이 어떻게 변화하는지 보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="c1"># 기울기 계산
</span>  <span class="n">d_weights_input_hidden</span><span class="p">,</span> <span class="n">d_bias_hidden</span><span class="p">,</span> <span class="n">d_weights_hidden_output</span><span class="p">,</span> <span class="n">d_bias_output</span> <span class="o">=</span> <span class="nf">backward_propagation</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">updated_hidden_activation</span><span class="p">,</span> <span class="n">updated_output</span><span class="p">,</span> <span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span>
  <span class="p">)</span>

  <span class="c1"># 가중치와 편향 업데이트
</span>  <span class="n">weights_input_hidden</span> <span class="o">=</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">d_weights_input_hidden</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
  <span class="n">bias_hidden</span> <span class="o">=</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">bias_hidden</span><span class="p">,</span> <span class="n">d_bias_hidden</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
  <span class="n">weights_hidden_output</span> <span class="o">=</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">d_weights_hidden_output</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
  <span class="n">bias_output</span> <span class="o">=</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">bias_output</span><span class="p">,</span> <span class="n">d_bias_output</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

  <span class="c1"># 순전파 재실행 (업데이트된 가중치 사용)
</span>  <span class="n">updated_output</span><span class="p">,</span> <span class="n">updated_hidden_activation</span> <span class="o">=</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">)</span>

  <span class="c1"># 업데이트된 예측값의 손실
</span>  <span class="n">updated_loss</span> <span class="o">=</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">updated_output</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">2</span><span class="si">}</span><span class="s">회차 업데이트된 Binary Cross-Entropy 손실 값:</span><span class="sh">"</span><span class="p">,</span> <span class="n">updated_loss</span><span class="p">)</span>
  <span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/가중치%20업데이트%20반복%20결과.png" alt="가중치 업데이트 반복 결과" /></p>
<p class="img_caption">가중치 업데이트 반복 결과</p>

<p>처음 실행했던 학습까지 포함해서 총 11회의 학습을 반복한 결과, 손실이 약 0.556까지 줄어들었습니다.</p>

<h2 id="tensorflow로-인공신경망-구현하기">TensorFlow로 인공신경망 구현하기</h2>

<p>지금까지 인공신경망의 학습 과정을 하나하나 구현해보았는데요, TensorFlow라는 라이브러리를 사용하면 인공신경망을 학습시키기 위한 각종 함수를 직접 구현하지 않고도 동일한 과정을 수행할 수 있습니다. Google이 개발한 TensorFlow는 딥러닝과 머신러닝 모델을 손쉽게 구축하고 학습시킬 수 있는 도구입니다. 딥러닝 모델 중 하나인 인공신경망도 TensorFlow를 활용하면 구현과 학습 과정이 굉장히 간단해집니다. TensorFlow를 활용해서 앞서 구현한 인공신경망과 동일한 구조의 모델을 구현해보겠습니다.</p>

<p>동일한 데이터셋(<code class="language-plaintext highlighter-rouge">df</code>)에서 독립변수와 종속변수를 분리하는 첫번째 과정은 동일하게 진행됩니다. 대신 데이터 정규화 과정에서는 또 다른 라이브러리, Scikit-learn(<code class="language-plaintext highlighter-rouge">sklearn</code>)을 사용했습니다. Scikit-learn은 데이터 전처리, 모델 학습, 평가 등 다양한 머신러닝 작업을 수행하는 파이썬 라이브러리입니다. 라이브러리를 사용하면 정규화를 위한 수식을 직접 구현하지 않고 해당 기능을 수행하는 모듈을 불러와 데이터 정규화와 같이 원하는 과정을 간단히 구현할 수 있죠.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># 독립변수(X)와 종속변수(y) 분리
</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="sh">"</span><span class="s">absences</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">night_study</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">club_activity</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">essay_length</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">exam_score</span><span class="sh">"</span><span class="p">]].</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">admission</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># 데이터 정규화
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<p>이제 데이터를 학습할 인공 신경망 모델을 구현해보겠습니다. 이전 과정(TensorFlow 없이 인공신경망을 구현)에서는 은닉층과 출력층의 가중치와 편향을 초기화하고, 순전파 함수와 활성화 함수 등을 정의함으로써 인공신경망을 구현했습니다. TensorFlow를 사용하면 인공신경망 모델을 하나의 객체로 구현할 수 있습니다. TensorFlow로 구현한 이번 인공신경망 객체 <code class="language-plaintext highlighter-rouge">model</code>은 이전 과정에서의 모델과 동일하게 다음과 같은 구조로 되어있습니다:</p>

<ul>
  <li>3개의 노드를 가진 하나의 은닉층</li>
  <li>활성화 함수: 은닉층 - ReLU, 출력층 - Sigmod</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TensorFlow 모델 정의
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="c1"># 입력층 -&gt; 은닉층 (5 -&gt; 3), 활성화 함수: ReLU
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,)),</span>
    <span class="c1"># 출력층 (3 -&gt; 1), 활성화 함수: Sigmoid
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>가중치와 편향값을 임의의 값으로 직접 초기화했던 이전 과정과 달리, TensorFlow를 사용하면 가중치와 편향은 각 층의 초기화 전략에 따라 초기화됩니다. 각 층의 특성에 맞게 효율적으로 초기화되도록 기본적으로 설정되어 있는거죠. 원한다면 <code class="language-plaintext highlighter-rouge">tf.keras.initializers</code>를 사용해서 초기화 방법을 지정할 수도 있습니다.</p>

<p>모델을 정의하고 나면, 학습률과 손실함수를 지정해 모델을 컴파일합니다. 이전 과정과 동일하게 학습률로는 0.01, 손실함수로는 Binary Cross-Entropy를 사용했습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 모델 컴파일
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>  <span class="c1"># 학습률 0.01로 설정
</span>              <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># 이진 분류를 위한 손실 함수
</span></code></pre></div></div>

<p>모델에 대한 설정을 마쳤으니, 미리 준비해둔 데이터(<code class="language-plaintext highlighter-rouge">X</code>와 <code class="language-plaintext highlighter-rouge">y</code>)를 전달해 학습을 진행하겠습니다. <code class="language-plaintext highlighter-rouge">model</code>객체에 <code class="language-plaintext highlighter-rouge">fit()</code>메소드를 적용합니다. 각 파라미터의 역할은 다음과 같습니다.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">X</code>, <code class="language-plaintext highlighter-rouge">y</code>: 학습 데이터의 독립변수와 종속변수</li>
  <li><code class="language-plaintext highlighter-rouge">epochs</code>: 반복 진행할 학습의 횟수</li>
  <li><code class="language-plaintext highlighter-rouge">batch_size</code>: 모델이 한 번에 처리할 학습 데이터의 개수. 크기가 작아질수록 가중치 업데이트가 세밀해지지만 학습 속도가 느려짐</li>
  <li><code class="language-plaintext highlighter-rouge">verbose</code>: 학습 중 출력되는 로그 메세지의 상세 수준</li>
</ul>

<p>아래는 모델 학습 코드를 실행한 뒤 출력되는 학습 과정의 로그 메세지입니다. 반복문으로 일련의 학습 과정을 반복했던 이전 과정과 달리 훨씬 간단해진 코드를 확인할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 모델 학습
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/posts/2024-12-09-ANN-2/tensorflow%20모델%20학습.png" alt="TensorFlow 모델 학습" width="400" /></p>
<p class="img_caption">TensorFlow 모델 학습</p>

<p>이렇게 TensorFlow를 활용하면 학습에 필요한 함수들을 일일이 정의하거나, 반복 학습을 위해 복잡한 함수들을 반복문 안에 직접 나열할 필요가 없습니다. 인공신경망을 직접 구현하며 그 동작 원리를 하나하나 이해하는 과정도 물론 중요하지만, 실제 프로젝트에서 모델을 활용할 때는 TensorFlow와 같은 잘 설계된 라이브러리를 사용해 중간 과정을 단순화하는 것이 효율적입니다.</p>

<h2 id="글을-마무리하며">글을 마무리하며..</h2>

<p>이번 글에서는 인공신경망(ANN)의 학습 과정을 이해하고, 학생의 학교 생활 데이터를 활용해 대입 합격 여부를 예측하는 모델을 직접 구축하고 학습시켜 보았습니다. ANN은 단순한 구조와 강력한 학습 능력으로 다양한 문제를 해결할 수 있지만, 시계열 데이터나 순차적 정보 처리에는 한계가 있습니다. 이러한 문제를 해결하기 위해 등장한 것이 순환 신경망(RNN)입니다. 또한 이미지나 영상 처리와 같이 공간적 관계를 파악해야 하는 문제에서는 합성곱 신경망(CNN)이 주로 사용됩니다. CNN은 이미지의 특징을 효과적으로 추출하고 학습할 수 있는 구조를 가지고 있어, 컴퓨터 비전과 같은 분야에서 큰 혁신을 가져왔습니다. 다음 글에서는 RNN의 기본 개념과 이를 개선한 LSTM 및 GRU를 활용한 시계열 문제 해결 방법을 다루고, CNN과 RNN의 차별성과 활용 사례를 비교해 깊이 있게 살펴보겠습니다.</p>]]></content><author><name>yunha</name></author><category term="Deep Learning 기초" /><category term="deep-learning" /><category term="ANN" /><category term="인공신경망" /><category term="역전파" /><category term="경사하강법" /><summary type="html"><![CDATA[인공신경망(Artificial Neural Network, ANN)은 데이터를 학습하고 패턴을 인식하며 새로운 데이터를 예측하는 혁신적인 알고리즘입니다. 앞선 글에서는 인공신경망의 역사와 개념을 살펴보며, 신경망이 어떻게 발전해 왔는지 이해했습니다.]]></summary></entry></feed>